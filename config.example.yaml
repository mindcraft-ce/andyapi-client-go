# Example configuration for andyapi provider client
andy_api_url: "http://localhost:8080"
# Optional API key if Andy API requires auth later
andy_api_key: ""

# Provider identification (purely informational for now)
provider: "local-llm"

# Heartbeat interval seconds
heartbeat_interval: 30
# Reconnect backoff seconds (max)
reconnect_max_backoff: 30
# Request timeout seconds (for local API completion requests)
request_timeout: 60

# Local OpenAI-compatible endpoints
endpoints:
  - id: "ollama"
    url: "http://localhost:11434"
    api_key: ""
    # Extra headers to add to all requests for this endpoint
    extra_headers:
      # X-Custom-Header: "value"
    # Extra params to add to all request payloads for this endpoint
    extra_params:
      # temperature: 0.7
  - id: "vllm"
    url: "http://localhost:8000"
    api_key: "sk-..."
    extra_headers: {}
    extra_params: {}

# Models this client provides. Each model has its own concurrency, context length and capabilities.
models:
  - name: "local-7b-chat"
    internal_id: "llama3:latest"
    endpoint_id: "ollama"
    system_prompt: "You are a helpful assistant."
    max_completion_tokens: 4096
    concurrent_connections: 4
    supports_embedding: false
    supports_vision: false
    fallback: false
    enabled: true
    # Model-specific extra headers (override endpoint headers)
    extra_headers: {}
    # Model-specific extra params (override endpoint params)
    extra_params:
      temperature: 0.7
  - name: "vision-multimodal-13b"
    internal_id: "llava:13b"
    endpoint_id: "ollama"
    max_completion_tokens: 8192
    concurrent_connections: 2
    supports_embedding: true
    supports_vision: true
    fallback: true
    enabled: false
    extra_headers: {}
    extra_params: {}
